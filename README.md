Gemma-3 270M â€” Training a Small Language Model From Scratch

This repository demonstrates how to build, train, and run inference on a ~270M-parameter decoder-only language model from scratch, inspired by modern transformer architectures (Gemma-style / GPT-style), using pure PyTorch and minimal abstractions.

The goal of this project is educational and engineering-focused: to understand every component of a modern SLM pipeline end-to-end â€” from dataset loading and tokenization to model definition, training, and inference.

ğŸš€ What This Project Is (and Isnâ€™t)

What it is

A from-scratch implementation of a transformer-based Small Language Model

Trained on the TinyStories dataset

Covers the entire training pipeline, not just inference

Explicit implementation of:

Tokenization

Causal attention

Transformer blocks

Training loop

Loss tracking

Text generation

What it is NOT

âŒ Not an official Google Gemma model

âŒ Not fine-tuning or LoRA

âŒ Not production-optimized

âŒ Not claiming SOTA performance

If youâ€™re looking for a copy-paste LLM wrapper, this repo is not for you.

ğŸ§  Model Overview

Architecture: Decoder-only Transformer (causal LM)

Parameter count: ~270M

Training objective: Next-token prediction

Tokenizer: Byte-level BPE (tiktoken)

Dataset: TinyStories

Framework: PyTorch

Training style: Full pre-training from scratch

The implementation intentionally avoids heavy frameworks (e.g. HuggingFace Trainer) to keep the logic transparent and inspectable.

ğŸ“‚ Project Structure
Gemma_3_270_M_Small_Language_Model.ipynb


This notebook contains the full pipeline:

Dataset loading

Tokenization

Inputâ€“output batch construction

Model architecture definition

Loss function

Training configuration

Pre-training loop

Loss visualization

Inference & text generation

ğŸ—ƒ Dataset

TinyStories
A synthetic dataset of short stories generated by large models and designed specifically for training small language models.

Why TinyStories?

Clean language

Simple grammar

Ideal for verifying whether a model truly learns structure

Low noise, fast iteration

Dataset source:

load_dataset("roneneldan/TinyStories")

ğŸ”¤ Tokenization

Uses tiktoken

Byte-level encoding

Explicit conversion from text â†’ token IDs

No hidden preprocessing steps

This choice mirrors real-world LLM tokenization while keeping control over the pipeline.

ğŸ§± Model Architecture

Key components implemented manually:

Token + positional embeddings

Multi-head self-attention

Causal masking

Feed-forward networks

Residual connections

Layer normalization

Output projection to vocabulary

Parts of the implementation are inspired by:

Andrej Karpathyâ€™s nanoGPT

Standard transformer literature

But no pretrained weights are used.

ğŸ‹ï¸ Training

Standard cross-entropy loss

Adam-based optimization

Manual training loop

Explicit loss tracking and plotting

This setup is intentionally simple and readable â€” the focus is correctness and understanding, not raw throughput.

ğŸ§ª Inference

After training, the model can:

Generate text autoregressively

Sample tokens step-by-step

Demonstrate learned syntax and story structure

Inference is implemented without helper libraries to show exactly how decoding works.

ğŸ“Š Results

Loss curves are plotted during training

Generated samples show coherent short-story structure

Model learns grammar, punctuation, and narrative flow

This confirms that the architecture and training pipeline are functioning correctly.

âš ï¸ Limitations (Be Honest)

Training is compute-intensive

Not optimized for large-scale deployment

No mixed-precision or distributed training

No RLHF or instruction tuning

Quality is limited by dataset size and compute

These are engineering trade-offs, not mistakes.

ğŸ¯ Why This Project Matters

Most people claiming to â€œbuild LLMsâ€:

Fine-tune a checkpoint

Call an API

Hide behind frameworks

This project proves:

You understand transformers at the mechanism level

You can train a language model end-to-end

You know what breaks when scale, data, or compute changes

That distinction matters.

ğŸ›  Requirements
pip install torch datasets tiktoken matplotlib


GPU strongly recommended.

ğŸ“Œ Future Improvements

Flash attention

Mixed-precision training

Larger datasets

Checkpoint saving & loading

Evaluation benchmarks

Instruction tuning

ğŸ‘¤ Author

Hamid Gholami
AI Engineer â€” NLP, LLMs, Generative Models

ğŸ“œ License

MIT License
Use it, modify it, break it, learn from it.
